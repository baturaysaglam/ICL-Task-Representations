{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batur/anaconda3/envs/fv/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.FV_utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.FV_utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.FV_utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.FV_utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.FV_utils.eval_utils import decode_to_vocab, sentence_eval\n",
    "\n",
    "from src.LTV_utils.data import set_seed, sample_data, forward_pass\n",
    "from src.LTV_utils.LTV import LearnableTaskVector\n",
    "from src.LTV_utils.extract_utils import get_attn_out, get_head_activations_on_prompt\n",
    "from src.LTV_utils.intervention_utils import ltv_intervention, ltv_intervention_natural_text\n",
    "from src.inference_utils import compute_perplexity\n",
    "\n",
    "GPU_IDX = 0\n",
    "SEED = 17\n",
    "\n",
    "device = torch.device(f\"cuda:{GPU_IDX}\")\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading:  EleutherAI/gpt-j-6b\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)\n",
    "EDIT_LAYER = 9\n",
    "\n",
    "n_layers = model_config['n_layers']\n",
    "resid_dim = model_config['resid_dim']\n",
    "n_heads = model_config['n_heads']\n",
    "head_dim = resid_dim // n_heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Compute task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = 'antonym'\n",
    "act_fn = None\n",
    "loss_fn = F.cross_entropy\n",
    "batch_size = 100\n",
    "\n",
    "dataset = load_dataset(task_name, root_data_dir='dataset_files', seed=0)\n",
    "mean_activations, _ = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ICL prompt:\n",
      " '<|endoftext|>Q: posterior\\nA: anterior\\n\\nQ: extinct\\nA: alive\\n\\nQ: latent\\nA: manifest\\n\\nQ: able\\nA: unable\\n\\nQ: expenditure\\nA: income\\n\\nQ: aware\\nA:' \n",
      "\n",
      "\n",
      "Shuffled ICL Prompt:\n",
      " '<|endoftext|>Q: posterior\\nA: manifest\\n\\nQ: extinct\\nA: unable\\n\\nQ: latent\\nA: alive\\n\\nQ: able\\nA: anterior\\n\\nQ: expenditure\\nA: income\\n\\nQ: aware\\nA:' \n",
      "\n",
      "\n",
      "Zero-Shot Prompt:\n",
      " '<|endoftext|>Q: aware\\nA:'\n"
     ]
    }
   ],
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "n_examples = 5\n",
    "test_idx = np.random.randint(0, len(dataset['test']))\n",
    "\n",
    "word_pairs = dataset['train'][np.random.choice(len(dataset['train']), n_examples, replace=False)]\n",
    "test_pair = dataset['test'][test_idx]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: posterior\\nA: anterior\\n\\nQ: extinct\\nA: alive\\n\\nQ: latent\\nA: manifest\\n\\nQ: able\\nA: unable\\n\\nQ: expenditure\\nA: income\\n\\nQ: aware\\nA:' \n",
      "\n",
      "Input Query: 'aware', Target: 'unaware'\n",
      "\n",
      "ICL Prompt Top K Vocab Probs:\n",
      " [(' unaware', 0.48833), (' unconscious', 0.11493), (' ignorant', 0.0853), (' oblivious', 0.06093), (' un', 0.05989)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt_seq_len = 5\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "path_to_model = os.path.join(os.path.dirname(current_directory), f\"language/src/LTV_models/{task_name}/{act_fn}/seq_len_{lt_seq_len}\")\n",
    "path_to_model = os.path.join(path_to_model, f\"ltv_layer_{lt_seq_len}.pth\")\n",
    "\n",
    "ltv_layer = LearnableTaskVector(n_layers, n_heads, head_dim).to(device)\n",
    "ltv_params = torch.load(path_to_model)\n",
    "ltv_layer.load_state_dict(ltv_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    attn_out = get_attn_out(mean_activations, model, model_config)\n",
    "    lt_vector = ltv_layer.forward(attn_out)\n",
    "    lt_vector = lt_vector.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the trained LTV Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: posterior\\nA: manifest\\n\\nQ: extinct\\nA: unable\\n\\nQ: latent\\nA: alive\\n\\nQ: able\\nA: anterior\\n\\nQ: expenditure\\nA: income\\n\\nQ: aware\\nA:' \n",
      "\n",
      "Input Query: 'aware', Target: 'unaware'\n",
      "\n",
      "Few-Shot-Shuffled Prompt Top K Vocab Probs:\n",
      " [(' unaware', 0.0995), (' aware', 0.06035), (' ignorant', 0.0587), (' unconscious', 0.05312), (' conscious', 0.05309)] \n",
      "\n",
      "Shuffled Prompt+FV Top K Vocab Probs:\n",
      " [(' unaware', 0.43724), (' ignorant', 0.09005), (' unconscious', 0.07406), (' oblivious', 0.04356), (' un', 0.04091)] \n",
      "\n",
      "Shuffled Prompt+LTV Top K Vocab Probs:\n",
      " [(' unaware', 0.64999), (' unconscious', 0.15438), (' un', 0.04982), (' ignorant', 0.03715), (' oblivious', 0.0216)] \n",
      "\n",
      "Vanilla transformer - loss: 2.3076 \t perplexity: 10.0502\n",
      "Function Vector - loss: 0.8273 \t perplexity: 2.2871\n",
      "Learnable Task Vector - loss: 0.4308 \t perplexity: 1.5385\n"
     ]
    }
   ],
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits_fv = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "_, interv_logits_ltv = ltv_intervention(shuffled_sentence, [test_pair['output']], lt_vector, model, model_config, \n",
    "                                                   tokenizer)\n",
    "                                                   \n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_fv, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+LTV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_ltv, tokenizer, k=5), '\\n')\n",
    "\n",
    "target_idx = tokenizer.encode(\" \" + test_pair['output'])\n",
    "target_idx = torch.tensor(target_idx, dtype=torch.int64).to(device)\n",
    "\n",
    "clean_loss, clean_perplexity = loss_fn(clean_logits, target_idx), compute_perplexity(clean_logits, target_idx)\n",
    "fv_loss, fv_perplexity = loss_fn(interv_logits_fv, target_idx), compute_perplexity(interv_logits_fv, target_idx)\n",
    "ltv_loss, ltv_perplexity = loss_fn(interv_logits_ltv, target_idx), compute_perplexity(interv_logits_ltv, target_idx)\n",
    "\n",
    "print(f\"Vanilla transformer - loss: {clean_loss:.4f} \\t perplexity: {clean_perplexity:.4f}\")\n",
    "print(f\"Function Vector - loss: {fv_loss:.4f} \\t perplexity: {fv_perplexity:.4f}\")\n",
    "print(f\"Learnable Task Vector - loss: {ltv_loss:.4f} \\t perplexity: {ltv_perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: '<|endoftext|>Q: aware\\nA:' \n",
      "\n",
      "Input Query: 'aware', Target: 'unaware'\n",
      "\n",
      "Zero-Shot Top K Vocab Probs:\n",
      " [(' aware', 0.17471), (' a', 0.02292), (' able', 0.0139), (' not', 0.01292), (' to', 0.01032)] \n",
      "\n",
      "Zero-Shot+FV Vocab Top K Vocab Probs:\n",
      " [(' aware', 0.18541), (' unaware', 0.11167), (' un', 0.03718), (' ignorant', 0.02702), (' not', 0.02203)] \n",
      "\n",
      "Zero-Shot+LTV Vocab Top K Vocab Probs:\n",
      " [(' unaware', 0.7543), (' un', 0.05105), (' aware', 0.03786), (' oblivious', 0.02072), (' ignorant', 0.01884)] \n",
      "\n",
      "Vanilla transformer - loss: 10.2966 \t perplexity: 29631.6250\n",
      "Function Vector - loss: 11.5369 \t perplexity: 102425.1094\n",
      "Learnable Task Vector - loss: 14.5160 \t perplexity: 2014770.8750\n"
     ]
    }
   ],
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits_fv = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "clean_output, interv_logits_ltv = ltv_intervention(zeroshot_sentence, [test_pair['output']], lt_vector, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_fv, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+LTV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_ltv, tokenizer, k=5), '\\n')\n",
    "\n",
    "target_idx = tokenizer.convert_tokens_to_ids(test_pair['output'])\n",
    "target_idx = torch.tensor([target_idx], dtype=torch.int64).to(device)\n",
    "\n",
    "clean_loss, clean_perplexity = loss_fn(clean_logits, target_idx), compute_perplexity(clean_logits, target_idx)\n",
    "fv_loss, fv_perplexity = loss_fn(interv_logits_fv, target_idx), compute_perplexity(interv_logits_fv, target_idx)\n",
    "ltv_loss, ltv_perplexity = loss_fn(interv_logits_ltv, target_idx), compute_perplexity(interv_logits_ltv, target_idx)\n",
    "\n",
    "print(f\"Vanilla transformer - loss: {clean_loss:.4f} \\t perplexity: {clean_perplexity:.4f}\")\n",
    "print(f\"Function Vector - loss: {fv_loss:.4f} \\t perplexity: {fv_perplexity:.4f}\")\n",
    "print(f\"Learnable Task Vector - loss: {ltv_loss:.4f} \\t perplexity: {ltv_perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
