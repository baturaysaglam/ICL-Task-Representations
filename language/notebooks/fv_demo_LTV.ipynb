{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import os, re, json\n",
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval\n",
    "\n",
    "from src.ltv_utils.data import set_seed, sample_data, forward_pass\n",
    "from src.ltv_utils.LTV import LearnableTaskVector\n",
    "from src.ltv_utils.extract_utils import get_attn_out, get_head_activations_on_prompt\n",
    "from src.ltv_utils.intervention_utils import ltv_intervention, ltv_intervention_natural_text\n",
    "\n",
    "GPU_IDX = 0\n",
    "SEED = 0\n",
    "\n",
    "device = torch.device(f\"cuda:{GPU_IDX}\")\n",
    "set_seed(SEED)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name, device=device)\n",
    "EDIT_LAYER = 9\n",
    "\n",
    "n_layers = model_config['n_layers']\n",
    "resid_dim = model_config['resid_dim']\n",
    "n_heads = model_config['n_heads']\n",
    "head_dim = resid_dim // n_heads"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Compute task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "task_name = 'antonym'\n",
    "act_fn = None\n",
    "loss_fn = F.cross_entropy\n",
    "batch_size = 100\n",
    "\n",
    "dataset = load_dataset(task_name, seed=0)\n",
    "mean_activations, _ = get_mean_head_activations(dataset, model, model_config, tokenizer)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "n_examples = 5\n",
    "test_idx = np.random.randint(0, len(dataset['test']))\n",
    "\n",
    "word_pairs = dataset['train'][np.random.choice(len(dataset['train']), n_examples, replace=False)]\n",
    "test_pair = dataset['test'][test_idx]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "lt_seq_len = 5\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "path_to_model = os.path.join(os.path.dirname(current_directory), f\"src/LTV_models/{task_name}/{act_fn}/seq_len_{lt_seq_len}\")\n",
    "path_to_model = os.path.join(path_to_model, f\"ltv_layer_{lt_seq_len}.pth\")\n",
    "\n",
    "ltv_layer = LearnableTaskVector(n_layers, n_heads, head_dim).to(device)\n",
    "ltv_params = torch.load(path_to_model)\n",
    "ltv_layer.load_state_dict(ltv_params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# batch_size = 100\n",
    "# mean_activations, data = get_mean_head_activations(dataset, model, model_config, tokenizer,\n",
    "#                                                        n_icl_examples=20, N_TRIALS=100,\n",
    "#                                                        batch_structure=True)\n",
    "# mean_activations = get_head_activations_on_prompt(shuffled_prompt_data, model, model_config, tokenizer)\n",
    "with torch.no_grad():\n",
    "    attn_out = get_attn_out(mean_activations, model, model_config)\n",
    "    lt_vector = ltv_layer.forward(attn_out)\n",
    "    lt_vector = lt_vector.squeeze()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the trained LTV Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits_fv = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "_, interv_logits_ltv = ltv_intervention(shuffled_sentence, [test_pair['output']], lt_vector, model, model_config, \n",
    "                                                   tokenizer)\n",
    "                                                   \n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_fv, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+LTV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_ltv, tokenizer, k=5), '\\n')\n",
    "\n",
    "target_idx = tokenizer.encode(\" \" + test_pair['output'])\n",
    "target_idx = torch.tensor(target_idx, dtype=torch.int64).to(device)\n",
    "clean_loss = loss_fn(clean_logits, target_idx).detach().item()\n",
    "fv_loss = loss_fn(interv_logits_fv, target_idx).detach().item()\n",
    "ltv_loss = loss_fn(interv_logits_ltv, target_idx).detach().item()\n",
    "\n",
    "print(f\"Clean loss: {clean_loss:.4f}\")\n",
    "print(f\"Function vector loss: {fv_loss:.4f}\")\n",
    "print(f\"Learnable task vector loss: {ltv_loss:.4f}\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "clean_logits.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "compute_perplexity(clean_logits, target_idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "def compute_perplexity(logits, target_index):\n",
    "    # logits: Tensor of shape [batch_size, num_tokens]\n",
    "    # target_index: Tensor of shape [batch_size] containing indices of the correct words\n",
    "    \n",
    "    # Step 1: Calculate the log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Step 2: Gather the log probabilities of the correct words\n",
    "    # target_index needs to be unsqueezed to use gather, which requires the index tensor to have the same dimensions as logits\n",
    "    log_probs_target = log_probs.gather(dim=-1, index=target_index.unsqueeze(-1)).squeeze(-1)\n",
    "    \n",
    "    # Step 3: Compute the average negative log probability\n",
    "    avg_neg_log_prob = -log_probs_target.mean()\n",
    "    \n",
    "    # Step 4: Calculate the perplexity\n",
    "    perplexity = torch.exp(avg_neg_log_prob)\n",
    "    \n",
    "    return perplexity"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "x = 0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "x += decode_to_vocab(clean_logits, tokenizer, k=1)[0][0].split(' ')[-1] == test_pair['output']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "test_pair['output']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits_fv = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "clean_output, interv_logits_ltv = ltv_intervention(zeroshot_sentence, [test_pair['output']], lt_vector, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_fv, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+LTV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits_ltv, tokenizer, k=5), '\\n')\n",
    "\n",
    "target_idx = tokenizer.convert_tokens_to_ids(test_pair['output'])\n",
    "target_idx = torch.tensor([target_idx], dtype=torch.int64).to(device)\n",
    "clean_loss = loss_fn(clean_logits, target_idx).detach().item()\n",
    "fv_loss = loss_fn(interv_logits_fv, target_idx).detach().item()\n",
    "ltv_loss = loss_fn(interv_logits_ltv, target_idx).detach().item()\n",
    "\n",
    "print(f\"Clean loss: {clean_loss:.4f}\")\n",
    "print(f\"Function vector loss: {fv_loss:.4f}\")\n",
    "print(f\"Learnable task vector loss: {ltv_loss:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "mean_activations = get_head_activations_on_prompt(prompt_data, model, model_config, tokenizer)\n",
    "attn_out = get_attn_out(mean_activations, model, model_config)\n",
    "lt_vector = ltv_layer.forward(attn_out).squeeze()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io_fv = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "_, io_ltv = ltv_intervention_natural_text(sentence, lt_vector, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io_fv.squeeze())))\n",
    "print(\"GPT-J+LTV:\", repr(tokenizer.decode(io_ltv.squeeze())))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
