{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92451ac2-fbc7-4ac2-93d0-99bb23aca2d8",
   "metadata": {},
   "source": [
    "### Start by importing the packages, and setting the GPU index and seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478049ba",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import argparse\n",
    "import sys\n",
    "\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from utils.experiment import *\n",
    "from utils.function_vector import *\n",
    "from utils.learnable_task_vector import *\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from utils.plot import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "sys.path.append(\"..\")  # Adds higher directory to python modules path\n",
    "\n",
    "sns.set_theme('notebook', 'darkgrid')\n",
    "palette = sns.color_palette('colorblind')\n",
    "\n",
    "sns.set(context='paper', style='ticks', palette='colorblind')\n",
    "\n",
    "SEED = 17\n",
    "GPU_IDX = 2\n",
    "\n",
    "\n",
    "def calculate_memory_allocation(tensor):\n",
    "    num_elements = tensor.numel()\n",
    "    element_size = tensor.element_size()\n",
    "    memory_allocation_bytes = num_elements * element_size\n",
    "    memory_allocation_mb = memory_allocation_bytes / (1024 ** 2)\n",
    "    return memory_allocation_mb"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9c64a15f-f273-4aa3-873a-2d4408b3cbad",
   "metadata": {},
   "source": [
    "### Configure the experiment\n",
    "##### Here, ``seq_len`` is the maximum length of the prompts that the model was trained on. ``long_seq_len`` is the prompt lengths during the inference stage. Therefore, we perform ours tests with ``long_seq_len > seq_len``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0f84576",
   "metadata": {},
   "source": [
    "# Set the variables\n",
    "task_name = \"linear_regression\"\n",
    "\n",
    "batch_size = 256\n",
    "seq_len = 71\n",
    "long_seq_len = 96\n",
    "\n",
    "act_fn = None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e16ae75-5838-4ada-8a5c-75cfa8ea646c",
   "metadata": {},
   "source": [
    "#### Configure the directories for loading/saving the models/figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f85d49",
   "metadata": {},
   "source": [
    "#  Set seeds\n",
    "set_seed(SEED)\n",
    "device = torch.device(f\"cuda:{GPU_IDX}\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Construct the running and saving paths\n",
    "figures_path, preds_path = prepare_save_dirs(os.path.join(task_name, \"results\"))\n",
    "run_path = os.path.join(f\"../models\", task_name, \"pretrained\")\n",
    "\n",
    "experiment_dir = f\"./LTV_models/{task_name}/{act_fn}/seq_len_{seq_len}\"\n",
    "\n",
    "if not os.path.exists(experiment_dir):\n",
    "    raise ValueError(\"Invalid sequence length\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0ae65186-a892-474a-bd62-bb2f30306d68",
   "metadata": {},
   "source": [
    "#### Initialize the trained model and obtain the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd8f1e9",
   "metadata": {},
   "source": [
    "# Prepare the model, get the task and data samplers, and save the parameters required later\n",
    "model, conf, task_sampler, covariate_sampler, params = prepare_model(run_path, batch_size, device)\n",
    "n_dims, resid_dim, n_layers, n_heads, head_dim = params"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "96059dae-474c-459d-a073-cf6113b90f06",
   "metadata": {},
   "source": [
    "#### Sample a batch of prompts of length ``seq_len`` to compute FV and LTV on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01655805-dba0-4614-953c-621c5b7caf23",
   "metadata": {},
   "source": [
    "task = task_sampler()\n",
    "xs_sample = covariate_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end).to(device)\n",
    "ys_sample = task.evaluate(xs_sample).to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fecb116f-c2d2-4e3c-a095-99b23080a348",
   "metadata": {},
   "source": [
    "#### Compute _indirect effects_ (required for FV, out-of-scope for us), and use it to compute FV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5447bc7-46cc-4706-b852-47e6d808a5df",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Only for visualization\n",
    "n_top_heads_visual = 10\n",
    "\n",
    "# Compute the indirect\n",
    "indirect_effect_mat = get_top_heads(model, xs_sample, ys_sample, n_layers, n_heads)\n",
    "indirect_effect_mat_np = indirect_effect_mat.cpu().data.numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e228454c-844b-4c8c-bfb7-29aef54e3f07",
   "metadata": {},
   "source": [
    "#### Initialize the trained LTV layer and compute LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15bb888-ce9a-4607-8684-9166e3a51387",
   "metadata": {},
   "source": [
    "params_path = os.path.join(experiment_dir, f\"ltv_layer_{seq_len}.pth\")\n",
    "\n",
    "ltv_layer = LearnableTaskVector(n_layers, n_heads).to(device)\n",
    "ltv_layer.load_state_dict(torch.load(params_path))\n",
    "\n",
    "model.eval()\n",
    "ltv_layer.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3799383a-2211-472a-a499-f5c6393461b3",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    attn_out = get_attn_outs(model, xs_sample, ys_sample, n_layers, n_heads, head_dim, resid_dim)\n",
    "    LTV = ltv_layer(attn_out)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dca1292-846d-4a3c-b4b1-7ccbaaae9715",
   "metadata": {},
   "source": [
    "attn_out.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "aa74aea5-a259-4de8-8d0f-018d03939695",
   "metadata": {},
   "source": [
    "#### Now, sample a batch of longer prompts with length ``long_seq_len`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09cecb6f-c023-4873-a1ab-99333cf57cb8",
   "metadata": {},
   "source": [
    "# Sample the data consisting of long prompts\n",
    "xs_long = covariate_sampler.sample_xs(b_size=batch_size, n_points=long_seq_len).to(device)\n",
    "ys_long = task.evaluate(xs_long).to(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d13c0b3f-5c48-433c-83f1-9024b184a6f4",
   "metadata": {},
   "source": [
    "### On-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19fae45-830d-4ca2-a2d3-d8661a938bf0",
   "metadata": {},
   "source": [
    "m = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "scale = 1.0\n",
    "L = [6, 7, 8]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = evaluate_model(model, model, xs_long, ys_long, L=1, FV=None, dummy=m, scale=0.0)\n",
    "    loss = distance(pred, ys_long).cpu().data.numpy()\n",
    "\n",
    "    pred_FV = evaluate_model(model, model, xs_long, ys_long, L=L, FV=FV, dummy=m, scale=scale)\n",
    "    loss_FV = distance(pred_FV, ys_long).cpu().data.numpy()\n",
    "\n",
    "    pred_LTV = evaluate_model_on_LTV(model, model, xs_long, ys_long, LTV=LTV, dummy=None, scale=1.0)\n",
    "    loss_LTV = distance(pred_LTV , ys_long).cpu().data.numpy()\n",
    "    \n",
    "    losses = [loss, loss_FV, loss_LTV]\n",
    "    legends = [\"Transformer\", r\"Transformer + $v$\", r\"Transformer + $v_\\theta$\"]\n",
    "\n",
    "lambda_label = r\"$\\lambda$\"\n",
    "title = r\"$\\mathbf{w}$, $\\mathbf{x}$ $\\sim$ $\\mathcal{N}(0, 1)$\"\n",
    "\n",
    "plot_transformer(losses,\n",
    "                 legends,\n",
    "                 title,\n",
    "                 x_label=\"# in-context examples\",\n",
    "                 y_label=\"mean-squared error\",\n",
    "                 baseline=None,\n",
    "                 save_path=os.path.join(figures_path, f\"result_1000\"),\n",
    "                 font_size=15,\n",
    "                 y_ticks_max=20,\n",
    "                 dpi=250,\n",
    "                 show=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "46d578aa-d536-4d55-9ac8-313a627cf12d",
   "metadata": {},
   "source": [
    "### Distributional shift - on prompts with the standard length ``seq_len``\n",
    "#### $\\mathcal{N}(0, 1) \\rightarrow \\mathcal{N}(-0.2, 1.25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a834c714-d2a1-4048-bf81-6b1aa2c4e705",
   "metadata": {},
   "source": [
    "mean = -0.75\n",
    "std_dev = 1.25\n",
    "\n",
    "w_b = torch.randn(batch_size, n_dims, 1)\n",
    "w_b = w_b.to(device)\n",
    "w_b_shifted = w_b * std_dev + mean\n",
    "\n",
    "xs = covariate_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end).to(device)\n",
    "ys = (xs @ w_b)[:, :, 0]\n",
    "ys_shifted = (xs @ w_b_shifted)[:, :, 0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80625a6e-30c9-46b6-9493-6cf5a4202685",
   "metadata": {},
   "source": [
    "m = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "scale = 1.0\n",
    "L = [6, 7, 8]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = evaluate_model(model, model, xs, ys_shifted, L=1, FV=None, dummy=m, scale=0.0)\n",
    "    loss = distance(pred, ys).cpu().data.numpy()\n",
    "\n",
    "    pred_FV = evaluate_model(model, model, xs, ys_shifted, L=L, FV=FV, dummy=m, scale=scale)\n",
    "    loss_FV = distance(pred_FV, ys).cpu().data.numpy()\n",
    "\n",
    "    pred_LTV = evaluate_model_on_LTV(model, model, xs, ys_shifted, LTV=LTV, dummy=None, scale=1.0)\n",
    "    loss_LTV = distance(pred_LTV , ys).cpu().data.numpy()\n",
    "    \n",
    "    losses = [loss, loss_FV, loss_LTV]\n",
    "    legends = [\"Transformer\", r\"Transformer + $v$\", r\"Transformer + $v_\\theta$\"]\n",
    "\n",
    "lambda_label = r\"$\\lambda$\"\n",
    "title = r\"$\\bf{dist}$ $\\bf{shift:}$ $\\mathcal{N}(0, 1)$ $\\rightarrow$ $\\mathcal{N}(-0.75, 1.25)$\"\n",
    "# title = r\"$\\bf{out-of-dist:}$ $\\mathcal{N}(0, 1)$ $\\rightarrow$ $U(-2, 2)$\"\n",
    "\n",
    "plot_transformer(losses,\n",
    "                 legends,\n",
    "                 title,\n",
    "                 x_label=\"# in-context examples\",\n",
    "                 y_label=\"mean-squared error\",\n",
    "                 baseline=None,\n",
    "                 save_path=os.path.join(figures_path, f\"result_1000\"),\n",
    "                 font_size=15,\n",
    "                 y_ticks_max=20,\n",
    "                 dpi=250,\n",
    "                 show=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f5c09dc5-a074-4c19-9f50-6ef0c967540b",
   "metadata": {},
   "source": [
    "### Out-of-distribution - on prompts with the standard length ``seq_len``\n",
    "#### $\\mathcal{N}(0, 1) \\rightarrow U(-2, 2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78d43ef0-f0f0-4a02-9bd6-8f60cc978e1c",
   "metadata": {},
   "source": [
    "w_b = torch.randn(batch_size, n_dims, 1)\n",
    "w_b = w_b.to(device)\n",
    "w_b_out = torch.rand(size=w_b.size()).to(device) * 4 - 2\n",
    "\n",
    "xs = covariate_sampler.sample_xs(b_size=batch_size, n_points=conf.training.curriculum.points.end).to(device)\n",
    "ys = (xs @ w_b)[:, :, 0]\n",
    "# ys_out = (xs @ w_b_out)[:, :, 0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16f4db1d-3a1e-4e8d-97eb-7c9c09303616",
   "metadata": {},
   "source": [
    "n_FV_heads = 35\n",
    "\n",
    "with torch.no_grad():\n",
    "    top_heads = top_heads_locations(indirect_effect_mat_np, n_FV_heads)\n",
    "    universal_mean_activations = get_universal_mean_act(model, xs, ys, n_layers, n_heads, head_dim)\n",
    "    FV = compute_function_vector(model, universal_mean_activations, top_heads, resid_dim, head_dim)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d289ec4-5a0a-4b55-b309-70bd47ed4f1e",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    attn_out = get_attn_outs(model, xs, ys, n_layers, n_heads, head_dim, resid_dim)\n",
    "    LTV = ltv_layer(attn_out)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e909adff-8685-4c5b-be5c-3be5b29481d4",
   "metadata": {},
   "source": [
    "m = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "scale = 1.0\n",
    "L = [6, 7, 8]\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = evaluate_model(model, model, xs, ys_out, L=1, FV=None, dummy=m, scale=0.0)\n",
    "    loss = distance(pred, ys).cpu().data.numpy()\n",
    "\n",
    "    pred_FV = evaluate_model(model, model, xs, ys_out, L=L, FV=FV, dummy=m, scale=scale)\n",
    "    loss_FV = distance(pred_FV, ys).cpu().data.numpy()\n",
    "\n",
    "    pred_LTV = evaluate_model_on_LTV(model, model, xs, ys_out, LTV=LTV, dummy=None, scale=1.0)\n",
    "    loss_LTV = distance(pred_LTV , ys).cpu().data.numpy()\n",
    "    \n",
    "    losses = [loss, loss_FV, loss_LTV]\n",
    "    legends = [\"Transformer\", r\"Transformer + $v$\", r\"Transformer + $v_\\theta$\"]\n",
    "\n",
    "lambda_label = r\"$\\lambda$\"\n",
    "title = r\"$\\bf{out-of-dist:}$ $\\mathcal{N}(0, 1)$ $\\rightarrow$ $U(-2, 2)$\"\n",
    "\n",
    "plot_transformer(losses,\n",
    "                 legends,\n",
    "                 title,\n",
    "                 x_label=\"# in-context examples\",\n",
    "                 y_label=\"mean-squared error\",\n",
    "                 baseline=None,\n",
    "                 save_path=os.path.join(figures_path, f\"result_1000\"),\n",
    "                 font_size=15,\n",
    "                 y_ticks_max=20,\n",
    "                 dpi=250,\n",
    "                 show=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0dbd4fab-9b12-4a7d-8139-57dc0c54f334",
   "metadata": {},
   "source": [
    "### [Optional] Not a direct comparison but compare the emphasize given to attention heads by FV and LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "665286b9-e470-4b05-909a-e23f9a719a76",
   "metadata": {},
   "source": [
    "ltv_layer_weights_np = ltv_layer.weights.cpu().data.numpy().reshape(indirect_effect_mat_np.shape)\n",
    "\n",
    "# Visualize the heatmap of the attention head significance found by FV and LTV on a (n_layers x n_heads) grid\n",
    "create_heatmap(indirect_effect_mat_np, row_labels=[str(i) for i in range(n_heads)],\n",
    "               col_labels=[str(i) for i in range(n_layers)], color_label='AIE (CIE)',\n",
    "               num_top_heads=n_top_heads_visual, row_label_overall='Head Index', col_label_overall='Layer', show=True)\n",
    "\n",
    "create_heatmap(ltv_layer_weights_np, row_labels=[str(i) for i in range(n_heads)],\n",
    "               col_labels=[str(i) for i in range(n_layers)], color_label='LTV Layer weights',\n",
    "               num_top_heads=n_top_heads_visual, row_label_overall='Head Index', col_label_overall='Layer', show=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ec5bc6-98bf-4535-8cbc-79c3a3ee30af",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2478fb-36d2-4b1d-9287-618206a2ad2f",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
