{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "from collections import deque\n",
    "import os, random, re, json\n",
    "import torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.extract_utils import get_mean_head_activations, compute_universal_function_vector, get_attn_out\n",
    "from src.utils.intervention_utils import fv_intervention_natural_text, function_vector_intervention, ltv_intervention\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer\n",
    "from src.utils.prompt_utils import load_dataset, word_pairs_to_prompt_data, create_prompt\n",
    "from src.utils.eval_utils import decode_to_vocab, sentence_eval"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "def sample_data(dataset, model, model_config, tokenizer, n_examples, batch_size):\n",
    "    mean_activations, data = get_mean_head_activations(dataset, model, model_config, tokenizer, n_icl_examples=n_examples, N_TRIALS=batch_size, batch_structure=True)\n",
    "    attn_out = get_attn_out(mean_activations, model, model_config)\n",
    "    set_of_word_pairs, test_pairs = data\n",
    "    \n",
    "    sentences, targets = [], []\n",
    "\n",
    "    for word_pairs, test_pair in zip(set_of_word_pairs, test_pairs):\n",
    "        prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "        sentence = create_prompt(prompt_data)\n",
    "        sentences.append(sentence)\n",
    "        targets.append(test_pair['output'])\n",
    "        \n",
    "    return attn_out, sentences, targets\n",
    "\n",
    "def forward_pass(model, model_config, tokenizer, vocab_size, batch_size, sentences, targets, lt_vector_batch):\n",
    "    logits = torch.zeros(batch_size, vocab_size).to(device)\n",
    "    clean_logits = torch.zeros_like(logits).to(device)\n",
    "    target_indices = torch.zeros(batch_size).to(device)\n",
    "\n",
    "    for i, (sentence, target, lt_vector) in enumerate(zip(sentences, targets, lt_vector_batch)):\n",
    "        clean_output, intervention_output = ltv_intervention(sentence, target, lt_vector, model, model_config, tokenizer, compute_nll=False, generate_str=False)\n",
    "        logits[i] = intervention_output\n",
    "        clean_logits[i] = clean_output\n",
    "        \n",
    "        target_indices[i] = tokenizer.convert_tokens_to_ids(target[0])\n",
    "        \n",
    "    return logits, clean_logits, target_indices"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "class LearnableTaskVector(nn.Module):\n",
    "    def __init__(self, n_layers, n_heads, n_head_dim, act_fn=None):\n",
    "        super(LearnableTaskVector, self).__init__()\n",
    "        # Initialize the weights using a uniform distribution between 0 and 1\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.n_head_dim = n_head_dim\n",
    "        self.weights = nn.Parameter(torch.randn(n_layers, n_heads))\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply sigmoid to constrain weights between 0 and 1\n",
    "        # x is the input tensor of shape [batch_size, n_layers, n_heads, n_head_dim]\n",
    "        # Reshape weight to shape [1, n_layers, n_heads, 1]\n",
    "        # Ensure the weight is properly broadcastable with x\n",
    "        batch_size = x.shape[0]\n",
    "        # normalized_weights = torch.sigmoid(self.weights).unsqueeze(-1)\n",
    "        normalized_weights = self.weights.unsqueeze(0).unsqueeze(-1)\n",
    "\n",
    "        weighted_sum = x * normalized_weights\n",
    "\n",
    "        # The result will have shape [batch_size, n_layers, n_heads * n_head_dim]\n",
    "        out = weighted_sum.view(batch_size, self.n_layers, self.n_heads * self.n_head_dim)\n",
    "        \n",
    "        return out"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "model_name = 'EleutherAI/gpt-j-6b'\n",
    "model, tokenizer, model_config = load_gpt_model_and_tokenizer(model_name)\n",
    "vocab_size = 50400\n",
    "# Disable gradient updates for all transformer parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "n_layers = model_config['n_layers']\n",
    "resid_dim = model_config['resid_dim']\n",
    "n_heads = model_config['n_heads']\n",
    "head_dim = resid_dim // n_heads\n",
    "device = model.device"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "lr = 2.5e-5\n",
    "\n",
    "ltv_layer = LearnableTaskVector(n_layers, n_heads, head_dim).to(torch.device(\"cuda\"))\n",
    "loss_fn = F.cross_entropy\n",
    "optimizer = optim.Adam(ltv_layer.parameters(), lr=lr)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "n_iter = int(2e5)\n",
    "batch_size = 32\n",
    "n_examples = 5\n",
    "verbose_freq = 1\n",
    "\n",
    "dataset = load_dataset('antonym', seed=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "lowest_val_loss = float('inf')  # init lowest validation loss\n",
    "loss_verbose = deque(maxlen=20)\n",
    "\n",
    "for iter_i in range(n_iter):\n",
    "    ltv_layer.train()  # Set the model to training mode\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    attn_out, sentences, targets = sample_data(dataset, model, model_config, tokenizer, n_examples, batch_size)\n",
    "    lt_vector_batch = ltv_layer.forward(attn_out)\n",
    "    logits, clean_logits, target_indices = forward_pass(model, model_config, tokenizer, vocab_size, batch_size, sentences, targets, lt_vector_batch)\n",
    "    \n",
    "    loss = loss_fn(logits, target_indices.to(torch.int64))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_verbose.append(loss.item())\n",
    "\n",
    "    # # Validation phase\n",
    "    # ltv_layer.eval()  # Set the model to evaluation mode\n",
    "    # total_val_loss = 0.0\n",
    "    # num_val_batches = 0\n",
    "    # \n",
    "    # with torch.no_grad():\n",
    "    #     for inputs, ys, xs_long, ys_long in val_dataloader:\n",
    "    #         inputs, ys = inputs.to(device), ys.to(device)\n",
    "    #         xs_long, ys_long = xs_long.to(device), ys_long.to(device)\n",
    "    # \n",
    "    #         ltv_out = ltv_layer(inputs)\n",
    "    #         ys_pred = add_learnable_task_vector(model, xs_long, ys_long, ltv_out, dummy_indices=None, scale=1.0)\n",
    "    #         ys_pred = ys_pred[:, -1].unsqueeze(dim=1)\n",
    "    # \n",
    "    #         val_loss = loss_fn(ys_pred, ys)\n",
    "    #         total_val_loss += val_loss.item()\n",
    "    #         num_val_batches += 1\n",
    "\n",
    "    # print(f'Epoch [{epoch_i+1}/{n_epochs}] - training loss: {average_training_loss:.4f}, validation loss: {average_val_loss:.4f}')\n",
    "\n",
    "    if iter_i % 20 == 0:\n",
    "        torch.save(ltv_layer.state_dict(), os.path.join(f\"./models/seq_len_{n_examples}\", f'ltv_layer_{n_examples}.pth'))\n",
    "        print(f'Epoch [{iter_i+1}/{n_iter}] - training loss: {np.mean(loss_verbose):.4f}')\n",
    "\n",
    "    # Save model if validation loss is the lowest\n",
    "    # if average_val_loss < lowest_val_loss:\n",
    "    #     lowest_val_loss = average_val_loss\n",
    "    #     torch.save(ltv_layer.state_dict(), os.path.join(experiment_dir, f'ltv_layer_{seq_len}.pth'))\n",
    "    #     print('Checkpoint saved with lowest validation loss')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and Compute task-conditioned mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "queries"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Rearrange the dataset - remove the test samples\n",
    "train_data = list(dataset['train'])\n",
    "valid_data = list(dataset['valid'])\n",
    "test_data = list(dataset['test'])\n",
    "\n",
    "all_data = train_data + valid_data + test_data\n",
    "random.shuffle(all_data)\n",
    "\n",
    "split_index = int(0.8 * len(all_data))\n",
    "\n",
    "new_train_set = all_data[:split_index]\n",
    "new_valid_set = all_data[split_index:]\n",
    "\n",
    "dataset = {\n",
    "    'train': new_train_set,\n",
    "    'valid': new_valid_set\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "n_examples = 5\n",
    "\n",
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# [num. blocks, \n",
    "\n",
    "mean_activations.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute function vector (FV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "FV, top_heads = compute_universal_function_vector(mean_activations, model, model_config, n_top_heads=10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Creation - ICL, Shuffled-Label, Zero-Shot, and Natural Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Sample ICL example pairs, and a test word\n",
    "dataset = load_dataset('antonym')\n",
    "word_pairs = dataset['train'][:5]\n",
    "test_pair = dataset['test'][21]\n",
    "\n",
    "prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True)\n",
    "sentence = create_prompt(prompt_data)\n",
    "print(\"ICL prompt:\\n\", repr(sentence), '\\n\\n')\n",
    "\n",
    "shuffled_prompt_data = word_pairs_to_prompt_data(word_pairs, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "shuffled_sentence = create_prompt(shuffled_prompt_data)\n",
    "print(\"Shuffled ICL Prompt:\\n\", repr(shuffled_sentence), '\\n\\n')\n",
    "\n",
    "zeroshot_prompt_data = word_pairs_to_prompt_data({'input':[], 'output':[]}, query_target_pair=test_pair, prepend_bos_token=True, shuffle_labels=True)\n",
    "zeroshot_sentence = create_prompt(zeroshot_prompt_data)\n",
    "print(\"Zero-Shot Prompt:\\n\", repr(zeroshot_sentence))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check model's ICL answer\n",
    "clean_logits = sentence_eval(sentence, [test_pair['output']], model, tokenizer, compute_nll=False)\n",
    "\n",
    "print(\"Input Sentence:\", repr(sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"ICL Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupted ICL Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform an intervention on the shuffled setting\n",
    "clean_logits, interv_logits = function_vector_intervention(shuffled_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(shuffled_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Few-Shot-Shuffled Prompt Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Shuffled Prompt+FV Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Intervention on the zero-shot prompt\n",
    "clean_logits, interv_logits = function_vector_intervention(zeroshot_sentence, [test_pair['output']], EDIT_LAYER, FV, model, model_config, tokenizer)\n",
    "\n",
    "print(\"Input Sentence:\", repr(zeroshot_sentence), '\\n')\n",
    "print(f\"Input Query: {repr(test_pair['input'])}, Target: {repr(test_pair['output'])}\\n\")\n",
    "print(\"Zero-Shot Top K Vocab Probs:\\n\", decode_to_vocab(clean_logits, tokenizer, k=5), '\\n')\n",
    "print(\"Zero-Shot+FV Vocab Top K Vocab Probs:\\n\", decode_to_vocab(interv_logits, tokenizer, k=5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sentence = f\"The word \\\"{test_pair['input']}\\\" means\"\n",
    "co, io = fv_intervention_natural_text(sentence, EDIT_LAYER, FV, model, model_config, tokenizer, max_new_tokens=10)\n",
    "\n",
    "\n",
    "print(\"Input Sentence: \", repr(sentence))\n",
    "print(\"GPT-J:\" , repr(tokenizer.decode(co.squeeze())))\n",
    "print(\"GPT-J+FV:\", repr(tokenizer.decode(io.squeeze())), '\\n')"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
